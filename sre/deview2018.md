# DEVIEW 2018: Search Reliability Engineering

https://tv.naver.com/v/4580088



## 1. 들어가며

- SRE의 업무는 소방관과 비슷하다. 하지만 더 중요한 것은 장애가 나지 않도록, 그리고 장애가 나더라도 대응이 쉽도록 평소에 잘 대비하는 것이다.

- 어떻게 하면 장애를 예방할 수 있을지, 어떻게 하면 비용 효율적으로 장애에 대응할 수 있을지를 고민함.
- 두가지 난제
  - 장애 예방 비용이 장애가 났을 때 발생하는 손해 비용보다 적다는 것을 증명해야함
  - 장애가 나지 않는다면 SRE 덕분인지, 그냥 안나는 건지 증명해야함

- 글로벌 기업에서의 SRE 패턴을 그대로 갖다 쓰기는 어려움. 자체적으로 고민하고 해결해나가고 있다.
- 네이버에서 SRE
  - 모든 검색서비스 정상 동작
  - 1년에 10분 이하 다운타임
  - 고비용 사후 처리보다 저비용 사전 예방
  - 위 목표 달성을 위한 모든 활동



## 2. 네이버 검색 시스템의 SRE

- SRE를 도입하기 전에도 SRE와 비슷한 업무는 있었다. 이전에는 성능 엔지니어링 이라고 불렀다.

- 그 때나 지금이나 모든 활동의 시작은 지표를 관찰하는 것이다.
- 서비스와 서버가 많아질수록 지표를 보기가 힘들어졌다. 효율적인 관리 필요.
  - 통일된 규칙으로 서비스 ID 발급
  - 구조 가시화 -> 트래픽 흐름을 한눈에 볼 수 있고 원인 추적에 도움됨
  - 성능 / 비용 측정
- 가용량 지표 개발
  - 어떤 서비스는 서버 한대 죽어도 괜찮은데 어떤 서비스는 문제 발생. 이런 지표를 어떻게 나타낼까?
  - 기존 가용량 지표는 `장애 시간 / 전체 시간`. 이 지표로 99.998%라면 굉장히 좋아 보이지만 이는 1년동안 10분 장애임. 네이버 검색 10분 장애는 대재난이므로 이 지표는 도움이 안됨.
  - 분산 시스템에 맞는 가용량 지표를 개발.
    - 부하증가배수: 한 노드가 죽었을 때 형제 노드들이 현재의 몇 배 부하를 받는가?
    - 최대가용배수: 한 노드가 현재의 몇 배까지의 부하를 받을 수 있는가?
    - 임계 상황: 부하증가배수 > 최대가용배수
- 부하증가배수와 최대가용배수를 이용하여 가용량 경보 시스템 구성
  - 임계 상황 발생 횟수가 90% 감소
  - 그 다음은? 임계 상황 발생 원인을 분석할 수 있게 여러 요인들의 대시보드 개발

- SRE 업무는 이런 선순환 사이클
  - 새로운 지표 개발 -> 관제 범위 확대 -> 시스템 개발 -> 새로운 방법론 개발 -> 새로운 지표 개발
- 사이클이 돌 때마다 장애는 줄어들고 모두가 행복해질 것만 같지만 현실은 그렇지 않다.
  - 감시 지표가 많아질 수록 경보의 수가 너무 많아짐. (경보 폭탄, 경보 피로) 이번엔 경보 수를 줄여보자.

- 경보는 4가지로 분류된다.

|             | 긴급 대응 필요                                        | 대응 불필요                                     |
| ----------- | ----------------------------------------------------- | ----------------------------------------------- |
| 경보 발생   | True Positive (실제 장애)                             | False Positive (시간이 흐르면 정상화 되는 상황) |
| 경보 미발생 | False Negative (장애가 발생했으나 경보가 울리지 않음) | True Negative                                   |

여기서 중요한 건 `False Positive`와 `False Negative` . `False Negative`는 있으면 안되는 경우임. `False Negative`를 줄이기 위해 `False Positive`를 늘리게 된다. 지난 3년간 한 번만 겪음.

- 경보 피로의 주범은 `False Positive`다. 이걸 어떻게 줄일까?
  - 그래프를 보고 경험적으로 판단. 개인마다 숙련도 차이가 발생.
  - 자동화하면 좋은데 모든 경우를 자동화하는 건 불가능함. 대신,
    - 빠른 대응을 위해 필요한 데이터를 미리 모아주고
    - 기본적인 상황 판단 자동화
- 자동 경보 분석 예
  - CPU 사용량이 높아져 경보 발생. 같은 시기에 캐시 히트율 감소. -> 캐시 리프레시로 인한 일시적 CPU 증가이며 장애 아님으로 판단. CPU 사용량 그래프와 캐시 히트율 그래프의 기울기를 분단위로 분석하여 기울기의 정도에 따라 1~7까지 번호를 매겨 같은 패턴인지 검사.
  - 복제 서버들 중에서 하나의 서버에만 장애가 났는지를 코사인 유사도로 자동 판단.

- 네이버 SRE 만의 핵심 철학 2가지
  - 거시적 관점의 문제 분석
    - 서비스 별로 다른 담당자가 문제를 분석하게 되면... A 서비스에 문제가 생겨서 A 서비스 담당자가 조사를 하니 B 서비스의 문제였음. B 서비스 담당자에게 전달해서 B 서비스 담당자가 조사해보니 C 서비스 문제였음 .. => 커뮤니케이션 비용 증가
    - 따라서 거시적 관점에서 한번에 봐야한다.
  - 블랙박스 모니터링
    -  이상 트래픽이 감지되면 서버의 로그를 보면서 분석하는 것이 아니라 다른 계층의 모니터링을 참고하여 상황 분석

- SRE vs 서비스 개발자

|                | SRE 스타일     | 개발자 스타일  |
| -------------- | -------------- | -------------- |
| 문제 분석 관점 | 거시적 (Macro) | 미시적 (Micro) |
| 장애 관제 방식 | 블랙박스       | 화이트박스     |

- 위 두가지 철학으로 장애 관제에 시간적, 비용적 효울성을 가져오고 각 서비스 담당자가 있음에도 불구하고 SRE 만의 가치를 확보할 수 있음.



## 3. 실제 사례 소개

- 일반적으로 검색 트래픽은 사람들의 생체 주기를 따라감
  - 평일에는 많고, 주말과 공휴일에는 줄어듦
  - 평일 오전, 오후에 증가 점심시간, 저녁시간에 감소
  - 주말과 공휴일엔 아침~저녁까지 트래픽이 유지됨
  - 이런 패턴과 다른 트래픽(anomaly)이 발생했다면 뭔가 특별한 사건이 있는 것
- Anomaly 예시
  - 포항 지진 당시 평소 대비 3배 트래픽 발생. 이후 여진과 수능 연기 발표에도 트래픽 증가
    - 특별한 사건에 의한 트래픽 증가는 주로 중복 검색이 많으므로 캐시 서버로 트래픽을 잘 처리해줘야함.
  - 스포츠 이벤트
    - 평창 동계 올림픽, FIFA 월드컵, 아시안 게임 ...
    - 스포츠 경기는 PC와 모바일 트래픽 양상이 다름.
      - PC는 경기가 끝날 때 까지 트래픽 감소
      - 모바일은 하프타임, 득점 등엔 트래픽 증가
  - 방송/연예인 이슈
    - 방송에서 언급된 연예인에 대한 검색량이 순간 급증. 이 경우도 캐시 서버 덕분에 장애로 번지진 않음.
  - 그 외 내부적 이유 때문에도 발생
    - 서비스 개편과 버전 변경
    - 부하 테스트
    - 지표 수집 문제



## 4. Search Reliability Engineer

- SRE 엔지니어의 심리적 안정감도 SRE에 영향을 주는 큰 요인
  - 새벽, 주말 장애 관제는 일주일 단위로 IC와 Scribe 로테이션
    - IC (Incident Commander)
      - 전체 incident 통제 역할
      - 상황 판단과 의사 결정
      - 리더 또는 외부 조직과 대화
    - Scribe
      - 서기 역할
      - 판단에 필요한 지표 수집
  - 장애를 외부에서도 확인하기 쉽게 챗봇을 많이 활용
- SRE 문화
  - 비난 없는 "사실 기반" 사후 분석
  - SRE 홍보 활동